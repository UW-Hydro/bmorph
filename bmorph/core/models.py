import os
import copy
import concurrent.futures
import random
import pandas as pd
import numpy as np
import xarray as xr
from joblib import dump, load
from functools import partial
from typing import List, Tuple

from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline


def elevation_scaler(x, feature_range=(0, 1), data_range=(-420, 8848)):
    '''
    MinMaxScaler for elevations on Earth
    Credit: @jhamman
    '''
    fmin, fmax = feature_range
    dmin, dmax = data_range
    scale = (fmax - fmin) / (dmax - dmin)
    x_scaled = scale * x + fmin - dmin * scale
    return x_scaled

def cube(x):
    """
    Taking a cube, for the default inverse transformers
    For some reason numpy doesn't like keyword arguments,
    so you can'd do np.power(x, x2=3). This is necessary
    to get around that, so that we can specify it as the
    ``inverse_func``.
    """
    return np.power(x, 3)


def fit_transformers(df: pd.DataFrame, custom_transformers: dict={}):
    """
    Fit function transformers for preprocessing data to train
    a machine learning model. Note that this does not actually do
    any data transformations, but merely fits the transformers
    and returns them for later application. Any variables not in
    the standard set provided here will simply be standardized

    Parameters
    ----------
    df:
        Dataframe with complete set of all data for training
        This should have a MultiIndex with levels ('site', 'time')
    custom_transformers:
        Any custom transformers to be applied. These will override
        the default ones specified here.

    Returns
    -------
    fit_transformers:
        Dictionary of transformers in the form {varname: fit_transformer}
    """
    # Default transformers
    transformers = {
        'raw_flow': Pipeline([
            ('log1p', FunctionTransformer(func=np.log1p, inverse_func=np.expm1,
                                          validate=False)),
            ('normalize', MinMaxScaler())
            ]),
        'target_flow': Pipeline([
            ('log1p', FunctionTransformer(func=np.log1p, inverse_func=np.expm1,
                                          validate=False)),
            ('normalize', MinMaxScaler())
            ]),
        'precipitation': Pipeline([
            ('cbrt', FunctionTransformer(func=np.cbrt, inverse_func=cube,
                                         validate=False)),
            ('normalize', MinMaxScaler())
            ]),
        'elevation': FunctionTransformer(elevation_scaler),
        'temperature': StandardScaler(),
        'contributing_area': MinMaxScaler(),
        'other': StandardScaler(),
        }
    # Check if any custom transformers were given
    for varname, transformer in custom_transformers.items():
        transformers[varname] = transformer
    # Fit the transformers on a per variable level
    fit_transformers = {}
    for key in df.columns:
        # Look for key in transformers, otherwise use 'other' as default
        fit_transformers[key] = transformers.get(key, transformers['other'])
        fit_transformers[key].fit(df[[key]])
    return fit_transformers


def apply_transformers(df: pd.DataFrame, fit_transformers: dict, inverse=False):
    """
    Applies transformers fit in the `fit_transformers` function.
    This does not mutate data in place.

    Parameters
    ----------
    df:
        Dataframe with complete set of all data for training
        This should have a MultiIndex with levels ('site', 'time')
    fit_transformers:
        Dictionary of transformers to be applied. They must be
        generated by the ``fit_transformers`` function or manually
        fit before this function can be used. Format is
        ``{variable: fit_transformer}``
    inverse:
        Whether to invert from the given transformers. This simply
        calls the ``inverse_transform`` method instead of ``transform``

    Returns
    -------
    out:
        Dataframe transformed by each of the transformers
    """
    out = pd.DataFrame(index=df.index)
    if not inverse:
        for key in df:
            key = str(key)
            out[key] = fit_transformers[key].transform(df[[key]])
    else:
        for key in df:
            key = str(key)
            out[key] = fit_transformers[key].inverse_transform(df[[key]])
    return out


def save_transformers(transformers: dict, path='.') -> dict:
    """Saves transformers to disk"""
    out_files = {}
    for name, tformer in transformers.items():
        of = f'{path}{os.sep}{name}-bmorph-transformer.joblib'
        dump(tformer, of)
        out_files[name] = of
    return out_files


def load_transformers(file_list: dict) -> dict:
    """Load transformers from disk"""
    transformers = {}
    for var, file in file_list.items():
        transformers[var] = load(file)
    return transformers


def split_train_test_sites(sites: List, train_frac: float=0.8) -> Tuple[List]:
    """
    Randomly partition sites into test and train samples.

    Parameters
    ----------
    sites:
        List of sites to partition
    train_frac:
        Fraction of sites to place into the training partition

    Returns
    -------
    train_sites, test_sites
    """
    n_train = int(train_frac * len(sites))
    shuffled = copy.copy(sites)
    random.shuffle(shuffled)
    return shuffled[:n_train], shuffled[n_train:]


def partition_train_test(df: pd.DataFrame,
                         train_frac: float=0.8) -> Tuple[pd.DataFrame]:
    """
    Randomly partition a dataframe into test and train dataframes
    based on randomly splitting site groupings.

    Parameters
    ----------
    df:
        Dataframe to partition. Should have MultiIndex with levels
        ('site', 'time')
    train_frac:
        Fraction of sites to place into the training partition

    Returns
    -------
    train_df, test_df
    """
    sites = np.unique(df.index.get_level_values('site'))
    train_sites, test_sites = split_train_test_sites(sites, train_frac)
    return df.loc[train_sites], df.loc[test_sites]


def make_lookback(df: pd.DataFrame, lookback: int=7) -> pd.DataFrame:
    """
    Create a dataset for training or applying a recurrent network.
    The returned dataset has dimensions ``(samples, lookback, features)``.
    Credit: @jhamman

    Parameters
    ----------
    df:
        Dataframe to create lookback for.
    lookback:
        Number of timesteps to create for the ``lookback`` dimension.

    Returns
    -------
    A new dataset with the newly created ``lookback`` dimension
    """
    coords = {'features': df.columns}
    da = xr.DataArray(df.values, dims=("samples", "features"), coords=coords)
    lba = da.rolling(samples=lookback).construct("lookback")
    lba.coords['lookback'] = np.linspace(-1 * (lookback - 1), 0,
                                         num=lookback, dtype=int)
    mask = lba.isnull().any(("lookback", "features"))
    lba = lba.where(~mask, drop=True)
    return lba.transpose("samples", "lookback", "features")


def prep_lstm_training_data(df: pd.DataFrame, lookback: int=7,
                            target_feature='target_flow') -> Tuple[np.ndarray]:
    """
    Takes a dataframe, create the lookback, and separate the
    training and target data.
    """
    lookback_ds = make_lookback(df, lookback)
    in_features = list(lookback_ds.features.values)
    in_features.remove(target_feature)
    lstm_features = lookback_ds.sel(features=in_features)
    lstm_target = (lookback_ds.sel(features=target_feature)
                              .isel(lookback=-1))
    return lstm_features, lstm_target


def make_metrics() -> List:
    from tensorflow.keras import backend

    # root mean squared error (rmse) for regression (only for Keras tensors)
    def rmse(y_true, y_pred):
        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))

    # mean squared error (mse) for regression  (only for Keras tensors)
    def mse(y_true, y_pred):
        return backend.mean(backend.square(y_pred - y_true), axis=-1)

    # coefficient of determination (R^2) for regression  (only for Keras tensors)
    def r_square(y_true, y_pred):
        SS_res = backend.sum(backend.square(y_true - y_pred))
        SS_tot = backend.sum(backend.square(y_true - backend.mean(y_true)))
        return 1 - SS_res / (SS_tot + backend.epsilon())

    def bias(y_true, y_pred):
        return backend.mean(y_pred) - backend.mean(y_true)
    metrics = {'rmse': rmse, 'mse': mse, 'r_square': r_square, 'bias': bias}
    return metrics


def make_callbacks(name: str) -> List:
    """Creates some utilty callbacks for use in training."""
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.callbacks import ModelCheckpoint
    mc = ModelCheckpoint(f'best_{name}.h5', monitor='val_loss', mode='min',
                         verbose=0, save_best_only=True)
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=25)
    return [es, mc]


def create_lstm(train_shape: tuple, depth: int=1,
                n_nodes: int=10, loss='mse', compilation_kwargs={}):
    """
    Helper function to create various LSTM models.

    Parameters
    ----------
    train_shape:
        Shape of the training data, should be following the use of the
        `make_lookback` function.
    depth:
        How deep to construct the LSTM
    n_nodes:
        How wide each layer of the LSTM should be
    loss:
        Loss function

    Returns
    -------
    model:
        The compiled tensorflow model (using the sequential API)
    """
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.constraints import NonNeg
    model = Sequential()
    if depth == 1:
        # If single layer, just create it
        model.add(LSTM(n_nodes, input_shape=(train_shape[1], train_shape[2])))
    else:
        # For deeper networks we need to set some additional parameters
        model.add(LSTM(n_nodes, input_shape=(train_shape[1], train_shape[2]),
                       return_sequences=True))
        for i in range(1, depth-1):
            model.add(LSTM(n_nodes, return_sequences=True))
        model.add(LSTM(n_nodes))

    # Output layer is just one value
    model.add(Dense(1, activation='relu', kernel_constraint=NonNeg()))
    model.compile(loss=loss, optimizer='adam', metrics=list(make_metrics().values()))
    return model


def create_bidirectional_lstm(train_shape: tuple, depth: int=1,
                              n_nodes: int=10, loss='mse', compilation_kwargs={}):
    """
    Helper function to create various bidirectional LSTM models.

    Parameters
    ----------
    train_shape:
        Shape of the training data, should be following the use of the
        ``make_lookback`` function.
    depth:
        How deep to construct the BiLSTM
    n_nodes:
        How wide each layer of the BiLSTM should be
    loss:
        Loss function

    Returns
    -------
    model:
        The compiled tensorflow model (using the sequential API)
    """
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.layers import Bidirectional
    from tensorflow.keras.constraints import NonNeg
    model = Sequential()
    if depth == 1:
        # If single layer, just create it
        model.add(Bidirectional(LSTM(n_nodes),
                  input_shape=(train_shape[1], train_shape[2])))
    else:
        # For deeper networks we need to set some additional parameters
        model.add(Bidirectional(LSTM(n_nodes, return_sequences=True),
                  input_shape=(train_shape[1], train_shape[2]),
                  ))
        for i in range(1, depth-1):
            model.add(Bidirectional(LSTM(n_nodes, return_sequences=True)))
        model.add(Bidirectional(LSTM(n_nodes)))

    # Output layer is just one value
    model.add(Dense(1, activation='relu', kernel_constraint=NonNeg()))
    model.compile(loss=loss, optimizer='adam', metrics=list(make_metrics().values()))
    return model


def run_predict(ds: xr.Dataset, transformer_files: List[str],
                model_file: str, lookback: int) -> np.ndarray:
    """
    Take the raw dataset from mizuRoute, with other necessary features
    and run the neural network to predict the bias corrected local flows.

    Parameters
    ----------
    ds:
        The mizuRoute output dataset
    transformer_files:
        Paths to all of the transformers that were used
        to transform the training data for the model
    model_file:
        Path to the tensorflow model
    lookback:
        The number of days of lookback that were used
        during training

    Returns
    -------
    np.ndarray:
        The predicted bias corrected local flows
    """
    # Cannot have tensorflow loaded iin before hand,
    # otherwise parallelism isn't possible
    from tensorflow.keras import backend as K
    from tensorflow.keras.models import load_model

    # Load in the model and prep the output data
    model = load_model(model_file, custom_objects=make_metrics())
    transformers = load_transformers(transformer_files)
    corrected_flows = 0.0 * ds['raw_flow'].isel(time=slice(lookback-1, None))
    target_flow_transformer = transformers.pop('target_flow')
    lookback_ds_list = []

    # Set up data that will go into the model
    for seg in ds['seg']:
        df = ds.sel(seg=seg, drop=True).to_dataframe()
        transformed = apply_transformers(df, transformers)
        lookback_ds_list.append(make_lookback(transformed, lookback))
    lookback_ds = xr.concat(lookback_ds_list, dim='samples')

    # Run the predict, invert the data transformation, and reshape it
    pred = model.predict(lookback_ds.values)
    target_flow = target_flow_transformer.inverse_transform(pred.reshape(-1, 1))
    corrected_flows.values = target_flow.reshape(corrected_flows.shape)

    # Clear the tensorflow session so things don't go wonky in other processes
    K.clear_session()
    return corrected_flows

def parallel_predict(ds: xr.Dataset, transformer_files: List[str],
                     model_file: str, lookback: int, chunk_size: int,
                     num_workers: int) -> xr.DataArray:
    """
    Run a complete prediction workflow in parallel.

    Parameters
    ----------
    ds:
        The mizuRoute output dataset
    transformer_files:
        Paths to all of the transformers that were used
        to transform the training data for the model
    model_file:
        Path to the tensorflow model
    lookback:
        The number of days of lookback that were used
        during training
    chunk_size:
        Number of stream segments to process at a time
    num_workers:
        Number of parallel processes to use for prediction

    Returns
    -------
    corrected_flows:
        The predicted bias corrected local flows ready
        to be rerouted through mizuRoute
    """
    raise ValueError('This code is currently broken, please use the `run_predict` function instead')
    ds = ds.chunk({'time': -1, 'seg': chunk_size})
    chunks = np.cumsum(ds.chunks['seg'])
    starts = chunks[:-1]
    stops = chunks[1:]
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as exe:
        out = []
        for start, stop in zip(starts, stops):
            seg = ds.sel(seg=slice(start, stop))
            out.append(exe.submit(run_predict, seg, transformer_files, model_file, lookback))
        concurrent.futures.wait(out)
    first = run_predict(ds.sel(seg=slice(0, starts[0])), transformer_files, model_file, lookback)
    results = [o.result() for o in out]
    corrected = xr.concat(results, dim='seg')
    corrected = xr.concat([first, corrected], dim='seg')
    corrected_flows /= ds['contributing_area']
    return corrected_flows
