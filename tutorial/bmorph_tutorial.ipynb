{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bmorph Tutorial: Getting your first bias corrections\n",
    "This notebook demonstrates how to organize your data for input to ``bmorph`` and how to use ``bmorph`` to bias correct. The notebook contains the same information as the [tutorial](https://bmorph.readthedocs.io/en/latest/bmorph_tutorial.html) page in the ``bmorph`` documentation.\n",
    "In this notebook, we will demonstrate how to perform four variations of bias correction. For more information about the details of each method, we refer you to the [bias correction page](https://bmorph.readthedocs.io/en/latest/bias_correction.html) page. The rest of the documentation for ``bmorph`` can be found [here](https://bmorph.readthedocs.io/en/latest/index.html). We also have a [preprint](http://doi.org/10.1002/essoar.10507877.1) that describes ``bmorph`` in more technical detail.\n",
    "\n",
    "The four variations that we will showcase here are:\n",
    "* Independent Bias Correction: Univariate (IBC_U) : IBC_U is the traditional bias correction method. This method can only be performed at sites with reference data. All sites are bias corrected independently of one another.\n",
    "* Independent Bias Correction: Conditioned (IBC_C) : IBC_C allows for correcting for specific biases that are process-dependent. This method can only be performed at sites with reference data. All sites are bias corrected independently of one another.\n",
    "* Spatially Consistent Bias Correction: Univariate (SCBC_U): SCBC_U corrects local flows at each river reach in the network, and then reroutes them to aggregate, producing bias corrected flows at locations of interest where reference data does and does not exist.\n",
    "* Spatially Consistent Bias Correction: Conditioned (SCBC_C): SCBC_C also corrects the local flows like SCBC_U, but allows for conditioning on dependent processes.\n",
    "\n",
    "# Expectations using this Tutorial\n",
    "\n",
    "Before we begin, we expect that anyone using this notebook is already familiar with the following:\n",
    "\n",
    "- Python 3 syntax and logic\n",
    "- [Jupyter Notebooks](https://jupyterlab.readthedocs.io/en/stable/user/notebook.html)\n",
    "- [numpy](https://numpy.org/)\n",
    "- [pandas](https://pandas.pydata.org/)\n",
    "- [xarray](http://xarray.pydata.org/en/stable/)\n",
    "- [matplotlib](https://matplotlib.org/)\n",
    "\n",
    "To route the flows through the channel network, which is an essential part of the spatially consistent bias correction (SCBC) methods, we use ``mizuRoute`` as the routing model. This routing model is already pre-installed in this tutorial environment, but you would need to make sure it is installed in your own environment if you are running ``bmorph`` elsewhere. Note that all your routing files will need to conform to ``mizuRoute`` requirements in that case. For more information we refer you to the ``mizuRoute`` documentation:\n",
    "- [mizuRoute](https://mizuroute.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "This tutorial also makes use of a number of other open source libraries & programs which, although no familiarity is required, may be of interest:\n",
    "- [geopandas](https://geopandas.org/)\n",
    "- [tqdm](https://tqdm.github.io/)\n",
    "- [dask](https://dask.org/)\n",
    "- [networkx](https://networkx.org/)\n",
    "- [scipy](https://www.scipy.org/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "While proficiency with each item is not required to understand the notebook, the details of these items will not be included in the tutorial itself.\n",
    "\n",
    "# Learning objectives & goals\n",
    "\n",
    "This tutorial mainly shows how to use bmorph.core.workflows and bmorph.core.mizuroute_utils to bias correct streamflow. By the end of this tutorial you will know\n",
    "\n",
    "- The data requirements to perform streamflow bias corrections with ``bmorph``\n",
    "- Input and output formats that ``bmorph`` expects\n",
    "- The meaning of parameters for algorithmic control of ``bmorph`` bias corrections\n",
    "- How to perform independent bias correction at locations with reference flows\n",
    "- How to perform spatially consistent bias corrections across a river network, as well as rerouting the corrected flows with mizuRoute\n",
    "- How to use the analysis, evaluation, and visualization tools built into ``bmorph``\n",
    "\n",
    "We expect that this tutorial will take approximately half an hour for someone familiar with Python.\n",
    "\n",
    "# Import Packages and Load Data\n",
    "\n",
    "We start by importing the necessary packages for the notebook and downloading and installing the data for the Yakima basin, which is our study area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# Set a bigger default plot size\n",
    "mpl.rcParams['figure.figsize'] = (10, 8)\n",
    "mpl.rcParams['font.size'] = 22\n",
    "\n",
    "# Import bmorph, and mizuRoute utilities\n",
    "import bmorph\n",
    "from bmorph.util import mizuroute_utils as mizutil\n",
    "from bmorph.evaluation.simple_river_network import SimpleRiverNetwork\n",
    "from bmorph.evaluation import plotting as bplot\n",
    "\n",
    "# Set the environment directory, this is a workaround for portability\n",
    "envdir = os.path.dirname(sys.executable)\n",
    "\n",
    "# Import pyproj and set the data directory, this is a workaround for portability\n",
    "import pyproj\n",
    "pyproj.datadir.set_data_dir(f'{envdir}/../share/proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting the sample data\n",
    "\n",
    "The following code cell has two commands preceded by `!`, which indicates that they are shell commands. They will download the sample data and unpackage it. The sample data can be viewed as a HydroShare resource [here](https://www.hydroshare.org/resource/fd2a347d34f145b4bfa8b6bff39c782b/). This cell may take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://www.hydroshare.org/resource/fd2a347d34f145b4bfa8b6bff39c782b/data/contents/bmorph_testdata.tar.gz\n",
    "! tar xvf bmorph_testdata.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset: The Yakima River Basin\n",
    "\n",
    "Before getting into how to run ``bmorph``, let's look at what is in the sample data. You will note that we now have a `yakima_workflow` directory downloaded in the same directory as this notebook (found by clicking on the `tutorial/` tab left by Binder or navigating to this directory in the file explorer of your choice). This contains all of the data that you need to run the tutorial. There are a few subdirectories:\n",
    "\n",
    " * `gis_data`: contains shapefiles, this is mainly used for plotting, not for analysis\n",
    " * `input`: this is the input meteorologic data, simulated streamflow to be corrected, and the reference flow dataset\n",
    " * `mizuroute_configs`: this is an empty directory that will automatically be populated with ``mizuRoute`` configuration files during the bias correction process\n",
    " * `output`: this is an empty directory that will be where the bias corrected flows will be written out to\n",
    " * `topologies`: this contains the stream network topologies that will be used for routing flows via mizuroute\n",
    "\n",
    "\n",
    "The Yakima River Basin is a 16 thousand square kilometer sub-basin of the Columbia River Basin located on the eastern slopes of the Cascade mountains in central Washington state. The Yakima River Basin has a strong gradient in hydroclimate from the headwaters to the outlet. The headwaters are characterized by the humid eastern slopes of the Cascade mountains and receive over 2500 mm of precipitation in an average year. The outlet at the confluence of the Yakima and Columbia Rivers is arid, receiving on average less than 250 mm of precipitation per year. This gradient in precipitation coincides with a large gradient in elevation, with the headwaters exceeding an elevation of 2000 meters and the outlet at just over 120 meters above mean sea level. Due to orographic effects in the headwaters, most of the precipitation falls as snow through the fall and winter months which drives a strong seasonal cycle in streamflow. \n",
    "\n",
    "In this tutorial, the Yakima River Basin is discretized into 285 sub-basins (HRUs) and 143 stream segments.\n",
    "\n",
    "# Setting up some metadata\n",
    "\n",
    "Next we set up the gauge site names and their respective river segment identification\n",
    "numbers, or `site`'s and `seg`'s using a python dictionary. This dictionary will be used throughout to ensure the data does\n",
    "not get mismatched. ``bmorph`` uses the convention:\n",
    "`site_to_seg = { site_0_name : site_0_seg, ..., site_n_name, site_n_seg}`\n",
    "We also create the inverse mapping (`seg_to_site`) and lists of the sites (`ref_sites`) and segments (`ref_segs`) for later use. In the next section, we will show you on the map where each of these sites are on the stream network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_to_seg = {'KEE' : 4175, 'KAC' : 4171, 'EASW': 4170, \n",
    "               'CLE' : 4164, 'YUMW': 4162, 'BUM' : 5231,\n",
    "               'AMRW': 5228, 'CLFW': 5224, 'RIM' : 5240,\n",
    "               'NACW': 5222, 'UMTW': 4139, 'AUGW': 594,  \n",
    "               'PARW': 588,  'YGVW': 584,  'KIOW': 581}\n",
    "\n",
    "seg_to_site = {seg: site for site, seg in site_to_seg.items()}\n",
    "ref_sites = list(site_to_seg.keys())\n",
    "ref_segs = list(site_to_seg.values())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the Yakima River Basin\n",
    "\n",
    "With our necessary metadata defined, we can now make some quick plots orienting you to the Yakima River Basin. To do so we will read in a network topology file as well as shapefiles for the region. We will make one plot which has the Yakima River Basin, along with stream network, subbasins, and gauged sites labeled. We will also plot a network diagram which displays in an abstract sense how each stream segment is connected. For the latter we use the [SimpleRiverNetwork](https://bmorph.readthedocs.io/en/latest/srn.html) that we've implemented in ``bmorph``. To set up the `SimpleRiverNetwork` we need the topology of the watershed (`yakima_topo`). Our river network and shapefiles are based on the [Geospatial Fabric](https://doi.org/10.5066/P971JAGF). In the Geospatial Fabric, rivers and streams are broken into segments, each with a unique identifier, as illustrated above in the `site_to_seg` definition. The locations of the gauged sites are shown in red, while all of the ungauged stream segments are shown in darker grey. The sub-basins for each stream segment are shown in lighter grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yakima_topo = xr.open_dataset('yakima_workflow/topologies/yakima_huc12_topology.nc').load()\n",
    "yakima_hru = gpd.read_file('./yakima_workflow/gis_data/yakima_hru.shp').to_crs(\"EPSG:4326\")\n",
    "yakima_seg = gpd.read_file('./yakima_workflow/gis_data/yakima_seg.shp').to_crs(\"EPSG:4326\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 9), gridspec_kw={'width_ratios': [1.5, 1]})\n",
    "axes[1].invert_xaxis() # flip to make nodes line up better with map\n",
    "\n",
    "# Plot the subbasins and stream segments\n",
    "ax = yakima_hru.plot(color='gainsboro', edgecolor='white', ax=axes[0])\n",
    "yakima_seg.plot(ax=ax, color='grey')\n",
    "\n",
    "# Plot the reference flow sites\n",
    "ref_lats = yakima_seg[yakima_seg['seg_id'].isin(ref_segs)]['end_lat']\n",
    "ref_lons = yakima_seg[yakima_seg['seg_id'].isin(ref_segs)]['end_lon']\n",
    "ref_names = [seg_to_site[s] for s in yakima_seg[yakima_seg['seg_id'].isin(ref_segs)]['seg_id']]\n",
    "ax.scatter(ref_lons, ref_lats, color='crimson', zorder=100, marker='s', label='Gauged locations')\n",
    "for name, lat, lon in zip(ref_names, ref_lats, ref_lons):\n",
    "    if name in ['AUGW', 'EASW']:\n",
    "        # Set labels at a slightly different position so we don't have overlaps\n",
    "        offset_x, offset_y = -0.16, -0.04\n",
    "    else:\n",
    "        offset_x, offset_y = 0.02, 0.02\n",
    "    ax.text(lon+offset_x, lat+offset_y, name, fontsize=10, color='white', weight='bold',\n",
    "            bbox=dict(boxstyle=\"round\", ec='crimson', fc='crimson', ),)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Now plot the abstracted river network, with gauged sites highlighted\n",
    "yakima_srn = SimpleRiverNetwork(yakima_topo)\n",
    "yakima_srn.draw_network(color_measure=yakima_srn.generate_node_highlight_map(ref_segs), \n",
    "                        cmap=mpl.cm.get_cmap('Set1_r'), ax=axes[1], node_size=60)\n",
    "plt.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the streamflow and associated meteorological data\n",
    "\n",
    "Now we load in the meteorological data that will be used for the conditional bias correction: daily minimum temperature (`tmin`), seasonal precipitation (`prec`),\n",
    "and daily maximum temperature (`tmax`). In principle, any variable can be used for conditioning, (e.g. snow water equivalent (SWE), groundwater storage, landscape slope angle, etc.). The meteorological data that we use here represents the average over each HRU. However, the bias correction is organized by stream segments. We will remap the conditioning variables onto the stream segments in a moment, so that they can be used in the bias correction process.\n",
    "\n",
    "Finally, we load the simulated flows and reference flows. \n",
    "``bmorph`` is designed to bias correct streamflow simulated with [mizuroute](https://mizuroute.readthedocs.io/en/latest/). \n",
    "We denote the simulated flows as the \"raw\" flows when they are uncorrected, and the flows that will be used to correct the raw flows as the \"reference\" flows. During the bias correction process ``bmorph`` will map the raw flow values to the reference flow values by matching their quantiles. \n",
    "In our case the reference flows are estimated no-reservoir-no-irrigation (NRNI) flows taken from the [River Management Joint Operating Committee (RMJOC)](https://www.bpa.gov/p/Generation/Hydro/Documents/RMJOC-II_Part_II.PDF).\n",
    "\n",
    "All of the datasets discussed are in the `xarray` [Dataset format](http://xarray.pydata.org/en/stable/user-guide/data-structures.html#dataset), which contains the metadata associated with the original NetCDF files. You can inspect the data simply by printing it out. For instance, here you can see that both the reference flows and raw flows (named `IRFroutedRunoff`, for \"Impulse Response Function routed runoff\" from mizuRoute) are in cubic meters per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteorologic data\n",
    "yakima_met = xr.open_dataset('yakima_workflow/input/yakima_met.nc').load()\n",
    "# Remove the 17* prefix, which was used to denote the domain covers the region 17 of the Hydrologic Unit Maps\n",
    "yakima_met['hru'] = (yakima_met['hru'] - 1.7e7).astype(np.int32)\n",
    "\n",
    "# Raw streamflows\n",
    "yakima_raw = xr.open_dataset('yakima_workflow/input/yakima_raw_flows.nc')[['IRFroutedRunoff', 'dlayRunoff', 'reachID']].load()\n",
    "# Update some metadata\n",
    "yakima_raw['seg'] = yakima_raw.isel(time=0)['reachID'].astype(np.int32)\n",
    "\n",
    "# Reference streamflows - this contains sites from the entire Columbia river basin, but we will select out only the `ref_sites`\n",
    "yakima_ref = xr.open_dataset('yakima_workflow/input/nrni_reference_flows.nc').rename({'outlet':'site'})[['seg', 'seg_id', 'reference_flow']]\n",
    "# Pull out only the sites in the Yakima basin\n",
    "yakima_ref = yakima_ref.sel(site=ref_sites).load()\n",
    "\n",
    "print('Reference flow units: ', yakima_ref['reference_flow'].units)\n",
    "print('Raw flow units: ', yakima_raw['IRFroutedRunoff'].units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from ``mizuroute`` output to ``bmorph`` format\n",
    "\n",
    "``mizuroute_utils`` is our utility module that will handle the conversion of ``mizuRoute`` outputs to the format that we need for ``bmorph``. \n",
    "We will use the `mizutil.to_bmorph` function to merge all of the data we previously loaded, and calculate some additional information to perform spatially consistent bias corrections (SCBC).\n",
    "For more information about how we perform SCBC see [the SCBC page in the documentation](https://bmorph.readthedocs.io/en/develop/bias_correction.html#spatial-consistency-reference-site-selection-cdf-blend-factor).\n",
    "Now we pass our data in to ``to_bmorph``, the primary utility function for automating ``bmorph`` pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yakima_met_seg = mizutil.to_bmorph(yakima_topo, yakima_raw, yakima_ref, yakima_met,  fill_method='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up ``bmorph`` configuration and parameters\n",
    "\n",
    "Before applying bias correction we need to specify some parameters and configure ``bmorph``.\n",
    "Returning to these steps can help fine tune your bias \n",
    "corrections for your study basin.\n",
    "\n",
    "The ``train_window`` is the time period that we will use to train the bias correction\n",
    "model. This is the time range that is representative of the\n",
    "basin's expected behavior that ``bmorph`` should mirror.\n",
    "\n",
    "The ``bmorph_window`` is the time period for which ``bmorph`` should be applied to\n",
    "bias correct the raw streamflow data.\n",
    "\n",
    "Lastly the ``reference_window`` is when the reference flows should be used to \n",
    "smooth the Cumulative Distribution function (CDF) of the bias corrected flows. \n",
    "This is recommended to be set equal to the ``train_window``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = pd.date_range('1981-01-01', '1990-12-30')[[0, -1]]\n",
    "reference_window = train_window\n",
    "apply_window= pd.date_range('1991-01-01', '2005-12-30')[[0, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``interval`` is the length of ``bmorph``'s application intervals, \n",
    "typically multiple years to preserve hydrologic \n",
    "relationships. Note that for ``pandas.DateOffset``, 'year' and 'years' \n",
    "are different and an 's' should always be included here for ``bmorph``\n",
    "to run properly, even for a single year.\n",
    "\n",
    "``overlap`` describes how many days the bias correction cumulative distribution function\n",
    "windows should overlap in total with each other. ``overlap`` is evenly distributed before\n",
    "and after this window. This is used to reduce discontinuities between application periods.\n",
    "Typical values are between 60 and 120 days.\n",
    "\n",
    "The two \"smoothing\" parameters are used to smooth the timeseries before the CDFs are computed\n",
    "and have two different uses. The `n_smooth_short` is used in the actual calculation of the CDFs which are used to perform the quantile mapping. Smoothing is used to ensure that the CDFs are smooth. Setting a very low value here may cause noisier bias corrected timeseries. Setting a very high value may cause the bias corrections to not match extreme flows. Typical values are from 7-48 days.\n",
    "\n",
    "`n_smooth_long` on the other hand is used to preserve long-term trends in mean flows from the raw flows. Typical values are 270 to 720 days. Using very low values may cause bias corrections to be degraded. This feature can be turned off by setting `n_smooth_long` to `None`.\n",
    "\n",
    "``condition_var`` names the variable to use in conditioning, such as maximum\n",
    "temperature (`tmax`), 90 day rolling total precipitation (`seasonal_precip`), or daily\n",
    "minimum temperature (`tmin`). At this time, only one conditioning\n",
    "meteorological variable can be used per ``bmorph`` execution. In this example,\n",
    "``tmax`` and ``seasonal_precip`` have been commented out to select ``tmin`` as\n",
    "the conditioning variable. If you wish to change this, be sure to either change which variables are commented out or change the value of ``condition_var`` itself. For now we will just use `tmin`, which is the daily minimum temperature. Our hypothesis in choosing `tmin` is that it will be a good indicator for errors in snow processes, which should provide a good demonstration for how conditional bias correction can modify flow timing in desirable ways.\n",
    "\n",
    "Further algorithmic controls can be used to tune the conditional bias correction as well. Here we use the histogram method for estimating the joint PDF, which is provided as `hist` as the `method`. We also have implemented a kernel density estimator which will be used if you set the `method` to `kde`. While `kde` tends to make smoother PDFs it comes with a larger computational cost. For both methods we specify the number of `xbins` and `ybins` which control how fine grained the joint PDFs should be calculated as. Setting a very high number here can potentially cause jumpy artifacts in the bias corrected timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bmorph parameter values\n",
    "interval = pd.DateOffset(years=5)\n",
    "overlap = 90\n",
    "n_smooth_long = 365\n",
    "n_smooth_short = 21\n",
    "\n",
    "# Select from the various available meteorologic fields for conditioning\n",
    "#condition_var = 'tmax'\n",
    "#condition_var = 'seasonal_precip'\n",
    "condition_var = 'tmin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify some configuration parameters for ``bmorph``'s conditional and univariate\n",
    "bias correction methods, respectively. \n",
    "\n",
    "``output_prefix`` will be used to write and load files according to the\n",
    "basin's name. Make certain to update this with the actual name of\n",
    "the basin you are analyzing so you can track where different files\n",
    "are written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_config = {\n",
    "    'data_path':  './yakima_workflow',\n",
    "    'output_prefix': \"yakima\",\n",
    "    'raw_train_window': train_window,\n",
    "    'ref_train_window': reference_window,\n",
    "    'apply_window': apply_window,\n",
    "    'interval': interval,\n",
    "    'overlap': overlap,\n",
    "    'n_smooth_long': n_smooth_long,\n",
    "    'n_smooth_short': n_smooth_short,\n",
    "    'condition_var': condition_var,\n",
    "    'method': 'hist',\n",
    "    'xbins': 100,\n",
    "    'ybins': 100,\n",
    "}\n",
    "\n",
    "univariate_config = {\n",
    "    'data_path':  './yakima_workflow',\n",
    "    'output_prefix': \"yakima\",\n",
    "    'raw_train_window': train_window,\n",
    "    'ref_train_window': reference_window,\n",
    "    'apply_window': apply_window,\n",
    "    'interval': interval,\n",
    "    'overlap': overlap,\n",
    "    'n_smooth_long': n_smooth_long,\n",
    "    'n_smooth_short': n_smooth_short,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You made it! Now we can actually bias correct with ``bmorph``! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent bias correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First off we run the Independent Bias Corrections. All the steps to do this are completely contained\n",
    "in the cell below. \n",
    "\n",
    "We run through each of the gauge sites and correct them \n",
    "individually. Since independent bias correction can only be performed\n",
    "at locations with reference data, corrections are only performed at\n",
    "the gauge sites with reference data. Because the bias corrections are performed independently at all the sites, we also do not need to reroute the flows using mizuRoute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_u_flows = {}\n",
    "ibc_u_mults = {}\n",
    "ibc_c_flows = {}\n",
    "ibc_c_mults = {}\n",
    "cond_vars = {}\n",
    "\n",
    "raw_flows = {}\n",
    "ref_flows = {}\n",
    "for site, seg in tqdm(site_to_seg.items()):\n",
    "    raw_ts =   yakima_met_seg.sel(seg=seg)['IRFroutedRunoff'].to_series()\n",
    "    train_ts = yakima_met_seg.sel(seg=seg)['IRFroutedRunoff'].to_series()\n",
    "    obs_ts =   yakima_met_seg.sel(seg=seg)['up_ref_flow'].to_series()\n",
    "    cond_var = yakima_met_seg.sel(seg=seg)[f'up_{condition_var}'].to_series()\n",
    "    ref_flows[site] = obs_ts\n",
    "    raw_flows[site] = raw_ts\n",
    "    cond_vars[site] = cond_var\n",
    "\n",
    "    ## IBC_U (Independent Bias Correction: Univariate)\n",
    "    ibc_u_flows[site], ibc_u_mults[site] = bmorph.workflows.apply_bmorph(\n",
    "        raw_ts, train_ts, obs_ts, **univariate_config)\n",
    "\n",
    "    ## IBC_C (Independent Bias Correction: Conditioned)\n",
    "    ibc_c_flows[site], ibc_c_mults[site] = bmorph.workflows.apply_bmorph(\n",
    "        raw_ts, train_ts, obs_ts, condition_ts=cond_var, **conditional_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatially consistent bias correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the spatially consistent bias correction, we will need to route the flows and we will rely on ``mizuRoute`` for this. Here we specify where the ``mizuroute`` executable is installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mizuroute_exe = f'{envdir}/route_runoff.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use ``run_parallel_scbc`` to do the rest. \n",
    "In the first cell we will run the spatially-consistent bias correction without any conditioning. \n",
    "The second cell will run the spatially-consistent bias correction with conditioning.\n",
    "This produced bias corrected flows at all 143 stream segments in the Yakima River Basin.\n",
    "Finally, we select out the corrected streamflows for both cases (with and without conditioning) to only contain the gauged sites.\n",
    "Selecting out only the gauged locations allows us to compare the spatially-consistent methods with the independent bias corrections.\n",
    "Finally we combine all the data into a single xarray `Dataset` to make analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCBC without conditioning\n",
    "unconditioned_seg_totals = bmorph.workflows.apply_scbc(yakima_met_seg, mizuroute_exe, univariate_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCBC with conditioning\n",
    "conditioned_seg_totals = bmorph.workflows.apply_scbc(yakima_met_seg, mizuroute_exe, conditional_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we select out our rerouted gauge site modeled flows.\n",
    "unconditioned_site_totals = {}\n",
    "conditioned_site_totals = {}\n",
    "for site, seg in tqdm(site_to_seg.items()):\n",
    "    unconditioned_site_totals[site] = unconditioned_seg_totals['IRFroutedRunoff'].sel(seg=seg).to_series()\n",
    "    conditioned_site_totals[site] = conditioned_seg_totals['IRFroutedRunoff'].sel(seg=seg).to_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything \n",
    "yakima_analysis = xr.Dataset(coords={'site': list(site_to_seg.keys()), 'time': unconditioned_seg_totals['time']})\n",
    "yakima_analysis['scbc_c'] = bmorph.workflows.bmorph_to_dataarray(conditioned_site_totals, 'scbc_c')\n",
    "yakima_analysis['scbc_u'] = bmorph.workflows.bmorph_to_dataarray(unconditioned_site_totals, 'scbc_u')\n",
    "yakima_analysis['ibc_u'] = bmorph.workflows.bmorph_to_dataarray(ibc_u_flows, 'ibc_u')\n",
    "yakima_analysis['ibc_c'] = bmorph.workflows.bmorph_to_dataarray(ibc_c_flows, 'ibc_c')\n",
    "yakima_analysis['raw'] = bmorph.workflows.bmorph_to_dataarray(raw_flows, 'raw')\n",
    "yakima_analysis['ref'] = bmorph.workflows.bmorph_to_dataarray(ref_flows, 'ref')\n",
    "yakima_analysis.to_netcdf(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And also write the output as CSV files\n",
    "yakima_analysis['scbc_c'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_scbc_c.csv')\n",
    "yakima_analysis['scbc_u'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_scbc_u.csv')\n",
    "yakima_analysis['ibc_u'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_ibc_u.csv')\n",
    "yakima_analysis['ibc_c'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_ibc_u.csv')\n",
    "yakima_analysis['raw'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_raw.csv')\n",
    "yakima_analysis['ref'].to_pandas().to_csv(f'./yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed_ref.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's take a look at our results\n",
    "\n",
    "If you look closely, the following plots are the same ones included in [Plotting](https://bmorph.readthedocs.io/en/latest/evaluation.html#plotting)! Because the plotting functions expect the variable `seg`, we will need to rename `site` to `seg` for them to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yakima_ds = xr.open_dataset(f'yakima_workflow/output/{univariate_config[\"output_prefix\"]}_data_processed.nc')\n",
    "yakima_ds = yakima_ds.rename({'site':'seg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a few sites and colors to plot for consistency. To simplify our plots, we will only focus on `scbc_c` in the dataset we just created. The methods do allow for multiple methods to be compared at once however, so we will still need to store the singular `scbc_c` in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_sites = ['KIOW','YUMW','BUM']\n",
    "select_sites_2 = ['KIOW','CLFW','BUM','UMTW']\n",
    "bcs = ['scbc_c', 'scbc_u', 'ibc_c', 'ibc_u']\n",
    "colors = ['grey', 'black', 'red', 'orange', 'purple', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the mean weekly flows for some of the sites in Yakima River Basin. You can change or add sites above, but we will start with a small number of sites to make the plots easier to follow.\n",
    "\n",
    "These averages can also be plotted for daily or monthly intervals for more or less granularity. You can also change the `reduce_func` to calculate any other statistic over the dataset (you might try `np.median` or `np.var` for instance). Don't forget to change the `statistic_label` for other measures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bplot.plot_reduced_flows(\n",
    "    flow_dataset=yakima_ds, \n",
    "    plot_sites=select_sites_2, \n",
    "    interval='week',\n",
    "    reduce_func=np.mean,\n",
    "    statistic_label='Mean',\n",
    "    raw_var='raw', raw_name=\"Uncorrected\",\n",
    "    ref_var='ref', ref_name=\"Reference\",\n",
    "    bc_vars=bcs, bc_names=[bc.upper() for bc in bcs],\n",
    "    plot_colors=colors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can see that the conditional corrections (`x_C` methods) have more accurate flow timings, particularly during the falling limb of the hydrograph. This hints that our hypothesis on correcting on daily minimum temperature would provide a good proxy for correcting snowmelt biases. We will explore this a little bit more later.\n",
    "\n",
    "We also see that generally the `SCBC_x` and `IBC_x` methods are fairly similar in the mean, with an exception at CLFW. Advantages of the SCBC method are that we produce bias corrections on every river reach and that it produces bias corrected incremental flows which are consistent across the network.\n",
    "\n",
    "## Scatter\n",
    "\n",
    "The scatterplot compares absolute errors in streamflow (Q) before and after bias correction. The 1 to 1 and -1 to 1 lines are plotted for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bplot.compare_correction_scatter(\n",
    "    flow_dataset= yakima_ds, \n",
    "    plot_sites = select_sites,\n",
    "    raw_var = 'raw', \n",
    "    ref_var = 'ref', \n",
    "    bc_vars = bcs, \n",
    "    bc_names = [bc.upper() for bc in bcs],\n",
    "    plot_colors = list(colors[2:]),\n",
    "    pos_cone_guide = True,\n",
    "    neg_cone_guide = True,\n",
    "    symmetry = False,\n",
    "    title = '',\n",
    "    fontsize_legend = 120,\n",
    "    alpha = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distribtutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distributions are another useful way to look at the streamflow values. Keep in mind that ``bmorph`` uses a form of quantile mapping to make the bias corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bplot.compare_mean_grouped_CPD(\n",
    "    flow_dataset= yakima_ds, \n",
    "    plot_sites = select_sites,\n",
    "    grouper_func = bplot.calc_water_year, \n",
    "    figsize = (60,40),\n",
    "    raw_var = 'raw', raw_name = 'Uncorrected',\n",
    "    ref_var = 'ref', ref_name = 'Reference',\n",
    "    bc_vars = bcs, bc_names = [bc.upper() for bc in bcs],\n",
    "    plot_colors = colors,\n",
    "    linestyles = 2 * ['-','-','-'],\n",
    "    markers = ['o', 'X', 'o', 'o', 'o', 'o'],\n",
    "    fontsize_legend = 90,\n",
    "    legend_bbox_to_anchor = (1.9,1.0)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is also capable of subsetting data by month should you want to compare only January flows for example. Because ``bmorph`` makes changes based on flow distributions, this plot is the closest to directly analyzing how the different methods correct flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple River Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot information of the SCBC across the simple river network. Let's look at mean percent difference between the bias-corrected and raw flows for both `SCBC_U` and `SCBC_C`. From the timeseries plots created earlier you might have noticed that the conditional bias corrections produced lower flows in the spring months. We will start by looking only at those months. You might try changing the season if you're interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season = 'MAM' # Choose from DJF, MAM, JJA, SON\n",
    "scbc_c = conditioned_seg_totals['IRFroutedRunoff']\n",
    "scbc_u = unconditioned_seg_totals['IRFroutedRunoff']\n",
    "raw = yakima_met_seg['IRFroutedRunoff']\n",
    "scbc_c_percent_diff = 100 * ((scbc_c-raw)/raw).groupby(scbc_c['time'].dt.season).mean().sel(season=season)\n",
    "scbc_u_percent_diff = 100 * ((scbc_u-raw)/raw).groupby(scbc_u['time'].dt.season).mean().sel(season=season)\n",
    "\n",
    "mainstream_map = yakima_srn.generate_mainstream_map()\n",
    "scbc_u_percent_diff = pd.Series(data=scbc_u_percent_diff.to_pandas().values, index=mainstream_map.index)\n",
    "scbc_c_percent_diff = pd.Series(data=scbc_c_percent_diff.to_pandas().values, index=mainstream_map.index)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,10))\n",
    "yakima_srn.draw_network(color_measure=scbc_u_percent_diff, cmap=mpl.cm.get_cmap('coolwarm_r'), node_size=40,\n",
    "                        with_cbar=True, cbar_labelsize=20, ax=axes[0], cbar_title='')\n",
    "axes[0].set_title('SCBC_U')\n",
    "yakima_srn.draw_network(color_measure=scbc_c_percent_diff, cmap=mpl.cm.get_cmap('coolwarm_r'), node_size=40,\n",
    "                        with_cbar=True, cbar_labelsize=20, ax=axes[1], cbar_title='Mean percent difference from raw flows(%)')\n",
    "axes[1].set_title('SCBC_C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can see that the main differences between the two methods was in modifying the headwater flows, which are at higher elevations and receive more precipitation. This aligns with our hypothesis that the daily minimum temperature would provide a good proxy for erros in snow processes.\n",
    "\n",
    "# Moving forward\n",
    "\n",
    "In this tutorial you have learned how to set up, perform bias corrections, and analyze them with bmorph. While this tutorial is meant to cover the essentials there are quite a few diversions/alternatives that you could try out before leaving. If you'd like to mess around a bit before moving on. For instance:\n",
    "- What happens if you conditionally bias correct on a different variable? Try `seasonal_precip`, or even implement a bias correction conditional on the month if you're feeling adventurous!\n",
    "- How do the smoothing parameters affect the bias corrected flows? Try a wide range of `n_smooth_short`, or try setting `n_smooth_long` to `None` to turn off the correction of the mean trend.\n",
    "- Try removing half of the gauged sites to see how it affects the spatially-consistent bias correction. You can do this by commenting out (or deleting) some of the entries in `site_to_seg` up at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f3870f67d182f2e264f74948103e35c18a48c989bcf8fae698e6abb46fb3d67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
